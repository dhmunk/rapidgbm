{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RapidGBM Documentation","text":""},{"location":"#what-is-rapidgbm","title":"What is RapidGBM?","text":"<p>RapidGBM is a powerful Python package designed to streamline the process of tuning LightGBM models using the optimization framework Optuna. With RapidGBM, you can effortlessly fine-tune hyperparameters to achieve optimal model performance using an automated machine learning (AutoML) approach.</p>"},{"location":"#key-features-and-why-use-rapidgbm","title":"Key Features and Why Use RapidGBM?","text":"<ul> <li>Effortless Tuning: RapidGBM automates the process of hyperparameter tuning for LightGBM models, reducing the need for manual experimentation.</li> <li>Optuna Integration: Leveraging the power of Optuna, RapidGBM efficiently searches through the hyperparameter space to find the best configuration for your model.</li> <li>AutoML Capabilities: With RapidGBM, you can quickly iterate through different hyperparameter configurations, accelerating the development of high-performing machine learning models.</li> <li>Compatibility: RapidGBM is designed to seamlessly integrate with existing Python workflows, making it easy to incorporate into your machine learning pipelines.</li> <li>Save Time: By automating the hyperparameter tuning process, RapidGBM saves you valuable time and resources, allowing you to focus on other aspects of your machine learning project.</li> <li>Improve Performance: With optimized hyperparameters, your LightGBM models can achieve higher levels of accuracy and predictive power, leading to better results in real-world applications.</li> <li>Simplicity: RapidGBM provides a user-friendly interface for hyperparameter tuning, making it accessible to both beginners and experienced machine learning practitioners.</li> </ul>"},{"location":"#who-should-use-rapidgbm","title":"Who Should Use RapidGBM?","text":"<p>RapidGBM is suitable for anyone working with LightGBM models who wants to streamline the hyperparameter tuning process. Whether you're a data scientist, machine learning engineer, or researcher, RapidGBM can help you achieve better results in less time.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>A big thanks to Danil Zherebtsov who originally created Verstack.LGBMTuner, which RapidGBM is based on.</p>"},{"location":"License/","title":"License","text":"<p>Information about the license under which RapidGBM is distributed</p> <p>Terms of use and redistribution</p> <p>RapidGBM is distributed under the MIT License.</p> <p>MIT License</p> <p>Copyright (c) 2020 DanilZherebtsov (name of package: Verstack.LGBMTuner) Modified 2024 Daniel Porsmose (renamed and modified codebase)</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#install-python","title":"Install Python","text":"<p>RapidGBM requires Python to be installed on your system. You can download and install Python from the official website.</p>"},{"location":"installation/#install-rapidgbm","title":"Install RapidGBM","text":"<p>You can install RapidGBM from Pypi using pip, Python's package manager. Open your terminal or command prompt and run the following command:</p> <pre><code>pip install rapidgbm \n</code></pre> <p>Requirements: - Python 3.9 or higher</p> <p>(This will download and install the latest version of RapidGBM and its dependencies.)</p>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<p>After installation, you can verify that RapidGBM is installed correctly by importing it in a Python script or interpreter:</p> <pre><code>import rapidgbm\n</code></pre> <p>If no errors occur, RapidGBM is successfully installed on your system.</p>"},{"location":"quickstart/","title":"Quickstart Guide","text":""},{"location":"quickstart/#installation","title":"Installation","text":"<pre><code>pip install rapidgbm\n</code></pre>"},{"location":"quickstart/#classification-example","title":"Classification Example","text":"<pre><code>import pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom rapidgbm import RapidGBMTuner\n\n# Load and prepare data\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# Initialize and fit RapidGBMTuner\ntuner = RapidGBMTuner(metric='auc', trials=4, verbosity=5, visualization=False)\ntuner.fit(X_train, y_train)\n\n\nAUC = roc_auc_score(y_test, tuner.predict_proba(X_test))\nprint(\"Area under the curve (AUC):\", AUC)\n</code></pre>"},{"location":"quickstart/#output","title":"Output","text":"<pre><code>Area under the curve (AUC): 0.9987421383647799\n</code></pre>"},{"location":"quickstart/#regression-example","title":"Regression Example","text":"<pre><code>import pandas as pd\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom rapidgbm import RapidGBMTuner\n\n# Load and prepare data\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# Initialize and fit RapidGBMTuner\ntuner = RapidGBMTuner(metric='mae', trials=4, verbosity=5, visualization=False)\ntuner.fit(X_train, y_train)\n\n\nmae = mean_squared_error(y_test, tuner.predict(X_test))\nprint(\"Mean squared error (MAE):\", mae)\n</code></pre>"},{"location":"quickstart/#output_1","title":"Output","text":"<pre><code>Mean squared error (MAE): 0.18446140275456893\n</code></pre>"},{"location":"reference/","title":"API-reference","text":"<p>This part of the project documentation focuses on an information-oriented approach. Use it as a reference for the technical implementation of the <code>calculator</code> project code.</p> <p>             Bases: <code>ArgValidationMixin</code>, <code>PlottingMixin</code></p>"},{"location":"reference/#rapidgbm.tuner.RapidGBMTuner.best_params","title":"<code>best_params: dict</code>  <code>property</code>","text":"<p>Best LGBM parameters found during the optimization process.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Best LGBM parameters</p>"},{"location":"reference/#rapidgbm.tuner.RapidGBMTuner.feature_importances","title":"<code>feature_importances: pd.Series</code>  <code>property</code>","text":"<p>Feature importances of the fitted model.</p> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Feature importances</p>"},{"location":"reference/#rapidgbm.tuner.RapidGBMTuner.fitted_model","title":"<code>fitted_model: lgb.Booster</code>  <code>property</code>","text":"<p>Fitted LGBM model object.</p> <p>Returns:</p> Type Description <code>Booster</code> <p>lgb.Booster: Fitted LGBM model</p>"},{"location":"reference/#rapidgbm.tuner.RapidGBMTuner.init_params","title":"<code>init_params: dict</code>  <code>property</code>","text":"<p>Initial LGBM parameters inferred based on data statistics and built-in logic.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Initial LGBM parameters</p>"},{"location":"reference/#rapidgbm.tuner.RapidGBMTuner.study","title":"<code>study: optuna.Study</code>  <code>property</code>","text":"<p>Optuna study object.</p> <p>Returns:</p> Type Description <code>Study</code> <p>optuna.Study: Optuna study</p>"},{"location":"reference/#rapidgbm.tuner.RapidGBMTuner.fit","title":"<code>fit(X, y, optuna_study_params=None)</code>","text":"<p>Fits the LightGBM model by finding optimized parameters based on the training data and metric.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The training features.</p> required <code>y</code> <code>Series</code> <p>The training labels.</p> required <code>optuna_study_params</code> <code>dict</code> <p>Parameters for the Optuna study. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the features or target arguments are invalid.</p> Example <pre><code>from rapidgbm import RapidGBMTuner\ntuner = RapidGBMTuner(metric='log_loss', trials=100, refit=True, verbosity=1)\ntuner.fit(X_train, y_train)\n</code></pre> Notes <ul> <li>The optimization metric is determined based on the metric specified during initialization.</li> <li>For regression, the optimization metric is selected based on the <code>eval_metric</code> parameter, except for 'r2'.     If <code>eval_metric</code> is 'r2', then the optimization metric is 'mean_squared_error'.</li> <li>For classification, the optimization metric is always 'log_loss'.</li> <li>The LGB Classifier/Regressor is inferred based on the metric and target variable statistics.</li> <li>Initial LGBM parameters are inferred based on data statistics and built-in logic, and can be accessed     using <code>self._init_params</code>.</li> <li>The parameter grid for hyperparameter search is inferred based on data statistics and built-in logic.</li> <li>The <code>optuna_study_params</code> parameter allows for customization of the Optuna study. Refer to the     documentation for <code>optuna.study.create_study</code> for more details.</li> </ul>"},{"location":"reference/#rapidgbm.tuner.RapidGBMTuner.fit_optimized","title":"<code>fit_optimized(X, y)</code>","text":"<p>Train model with tuned params on whole train data</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Train features.</p> required <code>y</code> <code>ndarray</code> <p>Train target.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <pre><code>from rapidgbm import RapidGBMTuner\ntuner = RapidGBMTuner(metric='log_loss', trials=100, refit=False, verbosity=1)\ntuner.fit(X_train, y_train)\ntuner.fit_optimized(np.array(X_train), np.array(y_train))\n</code></pre>"},{"location":"reference/#rapidgbm.tuner.RapidGBMTuner.plot_importances","title":"<code>plot_importances(feature_importances, n_features=15, figsize=(10, 6), display=True, dark=True, save=True)</code>","text":"<p>Plots the feature importances.</p> <p>Parameters:</p> Name Type Description Default <code>feature_importances</code> <code>Series</code> <p>The feature importances.</p> required <code>n_features</code> <code>int</code> <p>Number of features to plot. Defaults to 15.</p> <code>15</code> <code>figsize</code> <code>tuple</code> <p>Figure size. Defaults to (10,6).</p> <code>(10, 6)</code> <code>display</code> <code>bool</code> <p>Display the plot in a browser. If False, the plot will be saved in the current working directory. Defaults to True.</p> <code>True</code> <code>dark</code> <code>bool</code> <p>Display the dark or light version of the plot. Defaults to True.</p> <code>True</code> <code>save</code> <code>bool</code> <p>Save the plot to the current working directory. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"reference/#rapidgbm.tuner.RapidGBMTuner.plot_intermediate_values","title":"<code>plot_intermediate_values(study, legend=False, save=False, display=True)</code>","text":"<p>Plots the intermediate values of the study.</p> <p>Parameters:</p> Name Type Description Default <code>study</code> <code>Study</code> <p>The Optuna study containing the intermediate values.</p> required <code>legend</code> <code>bool</code> <p>Display the legend. Defaults to False.</p> <code>False</code> <code>save</code> <code>bool</code> <p>Save the plot as an image. Defaults to False.</p> <code>False</code> <code>display</code> <code>bool</code> <p>Display the plot. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"reference/#rapidgbm.tuner.RapidGBMTuner.plot_optimization_history","title":"<code>plot_optimization_history(study, save=False, display=True)</code>","text":"<p>Plots the optimization history of the parameters in the given Optuna study.</p> <p>Parameters:</p> Name Type Description Default <code>study</code> <code>Study</code> <p>The Optuna study containing the optimization history.</p> required <code>save</code> <code>bool</code> <p>Whether to save the plot as a PNG file. Defaults to False.</p> <code>False</code> <code>display</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"reference/#rapidgbm.tuner.RapidGBMTuner.plot_param_importances","title":"<code>plot_param_importances(study, save=False, display=True)</code>","text":"<p>Plots the parameter importances in the given Optuna study.</p> <p>Parameters:</p> Name Type Description Default <code>study</code> <code>Study</code> <p>The Optuna study containing the parameter importances.</p> required <code>save</code> <code>bool</code> <p>Whether to save the plot as an image file. Defaults to False.</p> <code>False</code> <code>display</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"reference/#rapidgbm.tuner.RapidGBMTuner.predict","title":"<code>predict(test, threshold=0.5)</code>","text":"<p>Predicts the target variable for the given test set using the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>test</code> <code>DataFrame</code> <p>The test features.</p> required <code>threshold</code> <code>float</code> <p>The binary classification probability threshold. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The predicted values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has not been fitted yet.</p> <code>TypeError</code> <p>If the test features are not a pandas DataFrame.</p> <code>ValueError</code> <p>If the threshold is not a float.</p> Example <pre><code>from rapidgbm import RapidGBMTuner\ntuner = RapidGBMTuner(metric='log_loss', trials=100, refit=True, verbosity=1)\ntuner.fit(X_train, y_train)\npredictions = tuner.predict(X_test, threshold=0.5)\n</code></pre>"},{"location":"reference/#rapidgbm.tuner.RapidGBMTuner.predict_proba","title":"<code>predict_proba(test)</code>","text":"<p>Predict probabilities for classification problems.</p> <p>Parameters:</p> Name Type Description Default <code>test</code> <code>DataFrame</code> <p>The test features.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If self._fitted_model.params['objective'] == 'regression', indicating that predict_proba() is only applicable for classification objectives.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The predicted probabilities.</p> Example <pre><code>from rapidgbm import RapidGBMTuner\ntuner = RapidGBMTuner(metric='log_loss', trials=100, refit=True, verbosity=1)\ntuner.fit(X_train, y_train)\npredictions = tuner.predict_proba(X_test)\n</code></pre>"},{"location":"reference/#rapidgbm.tuner.RapidGBMTuner.update_grid","title":"<code>update_grid(lgb_key, params)</code>","text":"<p>Change the grid for a specific LightGBM parameter.</p> <p>Parameters:</p> Name Type Description Default <code>lgb_key</code> <code>str</code> <p>The key of the LightGBM parameter.</p> required <code>params</code> <code>Union[list, tuple, dict]</code> <p>The new grid for the parameter. It can be passed as a list, tuple, or dict.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If params is not a list, tuple, or dict.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Notes <ul> <li>list (will be used for a random search)</li> <li>tuple (will be used to define the uniform grid range between the min(tuple), max(tuple))</li> <li>dict with keywords 'choice'/'low'/'high'</li> </ul> Example <pre><code>tuner = Tuner()\ntuner.update_grid('boosting_type', ['gbdt', 'rf'])  # random search\ntuner.update_grid('learning_rate', (0.001, 0.1))  # uniform grid range between the min(tuple), max(tuple))\ntuner.update_grid('num_leaves', {'low': 0.1, 'high': 5})  # uniform grid range between the low and high values\n</code></pre>"},{"location":"tutorials/","title":"Comming Soon","text":""}]}